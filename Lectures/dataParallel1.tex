
\begin{slide}
\heading{Introduction}

In this chapter we will study a particular style of data parallel
programming where the threads proceed \emph{synchronously}.  

The program proceeds in \emph{rounds}.  At the end of each round, there is a
global synchronisation, to ensure all threads have completed one round before
any proceeds to the next.
\end{slide}

%%%%%

\begin{slide}
\heading{Introduction}

Typically, each thread will operate on one section of the data, but  may
need to read data updated by other threads.  The synchronisation can be used
to ensure that one thread obtains the updates to the data made by other
threads on the previous round.

The data can be distributed between threads by two different techniques.
%
\begin{itemize}
\item
By sending messages; this works well when each piece of data has to be
passed to only a few other threads;

\item
By writing to shared variables.
%% ; in this case, there needs to be a global
%% synchronisation part way through each round, after threads have finished
%% reading the shared variables, and before they start writing to them.
\end{itemize}

%These algorithms are sometimes known as heart-beat algorithms.
\end{slide}

%%%%%

\begin{slide}
\heading{Applications}

\begin{itemize}
\item
Image processing;

\item
Solving differential equations, for example in weather forecasting or fluid
dynamics; 

\item
Matrix calculations;

\item
Cellular automata.
\end{itemize}
\end{slide}

%%%%%

\begin{slide}
\heading{Barrier synchronisation}

The global synchronisation at the end of each round is sometimes known as a
\emph{barrier synchronisation}, since it represents a barrier than no thread
may pass until all have reached that point.

Suppose we have |p| threads with identities $[\sm{0} \mathbin{..} \sm{p})$.
  Then a suitable barrier synchronisation object may be created by:
%
\begin{scala}
val barrier = new Barrier(p)
\end{scala}
%
%% (or |new Barrier(p)|)
%% where \SCALA{p} is the number of threads (with $\sm p > 1$).

A thread with identity~|me| performs the barrier synchronisation by executing
%
\begin{scala}
barrier.sync(me)
\end{scala}
%
No call to \SCALA{sync} will return until all \SCALA{p} threads have called
it. 
\end{slide}

%%%%%

\begin{slide}
\heading{A possible implementation of a barrier using a server}

\begin{scala}
class ServerBarrier(p: Int){
  private val arrive = ManyOne[Unit]
  private val leave = OneMany[Unit]

  def sync(me: Int) = { arrive!(); leave?() }

  private def server = thread{
    while(true){
      for(i <- 0 until p) arrive?()
      for(i <- 0 until p) leave!()
    }
  }

  server.fork
}
\end{scala}
\end{slide}

%%%%%


%%%%%

%% \begin{slide}
%% \heading{Testing the barrier implementation}

%% We can test the implementation using the technique of logging.  We arrange for
%% each of |p| workers to repeatedly: log that it is trying to synchronise; call
%% the |sync| operation; log that it has returned.

%% We then traverse the log, checking correctness.  At each point in the log, we
%% keep track of two (disjoint) sets of threads: (1)~those threads that are
%% trying to synchronise but cannot yet return, and (2)~those threads that may
%% return.  When the final thread tries to synchronise, we update to allow all
%% threads to return.  When a thread returns, we check that it is allowed to.

%% See the code on the course webpage.
%% \end{slide}

%%%%%

\begin{slide}
\heading{Barrier synchronisation}

With the actual implementation, each synchronisation takes time $O(\log
\sm{p})$.  (Creating the barrier object takes time $O(\sm{p})$.)

Exercise: implement a barrier with this running time.
\end{slide}

%%%%%

\begin{slide}
\heading{Example: prefix sums}

Suppose we have an array holding |n| integers:
\begin{scala}
val a = new Array[Int](n)
\end{scala}
%
We want to calculate the \emph{prefix sums} (i.e., the sums of the first $j$
elements, for $j = 1, \ldots, \sm n$) and store them in the array:
%
\begin{scala}
val sum = new Array[Int](n)
\end{scala}
%
So at the end we should have
\[
\forall k \in \set{0, \ldots, \sm n-1} \spot 
  \sm{sum}(k) = \textstyle\sum \sm{a}[0 \upto k] 
\]
We aim to do this using |n| threads, but in $\Theta(\log \sm n)$ rounds. 
\end{slide}

%%%%%

\begin{slide}
\heading{The idea of the algorithm}

Each thread~|me| sets $\sm{sum}(\sm{me})$ to $\sum \sm a[0 \upto \sm{me}]$.

The program proceeds in rounds, with a barrier synchronisation at the end of
each round.

Each thread will have a local variable~|s|, which is the sum it has
calculated so far.  At the end of round~$r$, thread~|me| will have 
\[
\sm s = \textstyle\sum \sm a(\sm{me}-\sm{gap} \upto \sm{me}]
   \mbox{ where } \sm{gap} = 2^r
\]
(and we invent fictitious elements $\sm a(i)=0$ for $i<0$).

On each round, thread~|me| obtains the value of thread
$(\sm{me}-\sm{gap})$'s |s|, which is
$\sum \sm a(\sm{me}-2.\sm{gap} \upto \sm{me}-\sm{gap}]$, and so calculates
$\sum \sm a(\sm{me}-2.\sm{gap} \upto \sm{me}]$, to maintain the above
invariant.

This continues until $\sm{gap} \ge \sm n$.
\end{slide}

%%%%%

\begin{slide}
\heading{Calculating prefix sums}

We will calculate the prefix sums using an object with the following
interface, and using |n| threads. 
%
\begin{scala}
class PrefixSums(n: Int, a: Array[Int]){
  require(n == a.size)

  /** Calculate the prefix sums. */
  def apply(): Array[Int] = ...
}
\end{scala}
\end{slide}

%%%%%

\begin{slide}
\heading{Synchronisation and communication}

The algorithm proceeds in rounds, so we need a barrier synchronisation object:
%
\begin{scala}
  /** Barrier synchronisation object. */
  private val barrier = new Barrier(n)
\end{scala}
%
We need channels to send values to threads:
\begin{scala}
  /** Channels on which values are sent, indexed by receiver's identity. */
  private val toSummers = Array.fill(n)(new BuffChan[Int](1))
\end{scala}
%
These are buffered to allow the send and receive to be asynchronous.  

Threads end up writing their prefix sums into the following array. 
\begin{scala}
  /** Shared array, in which sums are calculated. */
  private val sum = new Array[Int](n)
\end{scala}
\end{slide}

%%%%%

\begin{slide}
\heading{A thread}

\begin{scala}
  private def summer(me: Int) = thread("summer"+me){
    // Invariant: gap = £$2^r$£ and s = £$\sum$£ a(me-gap .. me]
    // (with fictious values a(i) = 0 for i < 0).  r is the round number.
    var r = 0; var gap = 1; var s = a(me)

    while(gap < n){
      if(me+gap < n) toSummers(me+gap)!s // pass my value up the line
      if(gap <= me){                    // receive from me-gap,
	val inc = toSummers(me)?()  // inc = £$\sum$£ a(me-2*gap .. me-gap]
	s = s + inc                     // s = £$\sum$£ a(me-2*gap .. me]
      }
      r += 1; gap += gap             // s = £$\sum$£ a(me-gap .. me]
      barrier.sync(me)
    }
    sum(me) = s
  }
\end{scala}
\end{slide}

%%%%%

\begin{slide}
\heading{Calculating the prefix sums}

\begin{scala}
  /** Calculate the prefix sums. */
  def apply(): Array[Int] = {
    run(|| (for (i <- 0 until n) yield summer(i)))
    sum
  }
\end{scala}
\end{slide}

%%%%%

\begin{slide}
\heading{The need for a barrier synchronisation}

The use of the barrier synchronisation ensures that different
threads hold the same value for the round number, and hence each thread can
depend upon the value it receives from its peer.

Suppose we didn't use a barrier synchronisation.  Consider a particular thread
|me| on round |r|, and let $\sm{gap} = 2^{\ss r}$.  It expects to receive from
thread~|me-gap|; but suppose that thread is slow.  And suppose
thread~|me-2.gap| is fast and has proceeded to round~|r+1|.  Then thread~|me|
will instead receive from thread~|me-2.gap|.  In particular, that means the
value it sends on the next round will be incorrect.
\end{slide}

%%%%%

\begin{slide}
\heading{Complexity}

The algorithm uses $\Theta(\log n)$ rounds.  Each barrier synchronisation
takes $\Theta(\log n)$ time.  This makes $\Theta((\log n)^2)$ in total
(ignoring the initialisation time).

\bigskip

\heading{Testing}

We can test this against a sequential implementation. 

% \emph{Question:} What goes wrong if we don't have the barrier
% synchronisation?

\bigskip

\heading{Exercise} 

Adapt the program so that the threads communicate via shared variables rather
than channels.
\end{slide}

%%%%%

\begin{selfnote}
Without the barrier synchronisation, a summer could receive values out of
order.  For example, \SCALA{Summer(3)} could first receive \SCALA{a(0)+a(1)}
from \SCALA{Summer(1)}, and then receive \SCALA{a(2)} from \SCALA{Summer(2)},
if Summer(1) happens to be faster than Summer(2).  Despite this
\SCALA{Summer(3)} ends up with the right answer; but it will pass
\SCALA{a(0)+a(1)+a(3)} to \SCALA{Summer(5)} in the second step (instead of
\SCALA{a(2)+a(3)}), so \SCALA{Summer(5)} ends up with the wrong value.
\end{selfnote}
